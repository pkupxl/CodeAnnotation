{
	"DataSyncRepCancelWait":{
		"body":"static void DataSyncRepCancelWait(void)\r\n{\r\n    (void)LWLockAcquire(DataSyncRepLock, LW_EXCLUSIVE);\r\n    if (!SHMQueueIsDetached(&(t_thrd.proc->dataSyncRepLinks)))\r\n        SHMQueueDelete(&(t_thrd.proc->dataSyncRepLinks));\r\n    t_thrd.proc->dataSyncRepState = SYNC_REP_NOT_WAITING;\r\n    LWLockRelease(DataSyncRepLock);\r\n}",
		"comment":"/*\r\n * Acquire DataSyncRepLock and cancel any wait currently in progress.\r\n */",
		"filename":"D:\\标注竞赛\\openGauss-server\\src\\gausskernel\\storage\\replication\\datasyncrep.cpp",
		"name":"DataSyncRepCancelWait"
	},
	"DataSyncRepQueueInsert":{
		"body":"static void DataSyncRepQueueInsert(void)\r\n{\r\n    PGPROC* proc = NULL;\r\n\r\n    proc = (PGPROC*)SHMQueuePrev(&(t_thrd.datasender_cxt.DataSndCtl->SyncRepQueue),\r\n        &(t_thrd.datasender_cxt.DataSndCtl->SyncRepQueue),\r\n        offsetof(PGPROC, dataSyncRepLinks));\r\n    while (proc != NULL) {\r\n        /*\r\n         * Stop at the queue element that we should after to ensure the queue\r\n         * is ordered by Position.\r\n         */\r\n        if (DQByteLT(proc->waitDataSyncPoint, t_thrd.proc->waitDataSyncPoint))\r\n            break;\r\n\r\n        proc = (PGPROC*)SHMQueuePrev(&(t_thrd.datasender_cxt.DataSndCtl->SyncRepQueue),\r\n            &(proc->dataSyncRepLinks),\r\n            offsetof(PGPROC, dataSyncRepLinks));\r\n    }\r\n\r\n    if (proc != NULL)\r\n        SHMQueueInsertAfter(&(proc->dataSyncRepLinks), &(t_thrd.proc->dataSyncRepLinks));\r\n    else\r\n        SHMQueueInsertAfter(&(t_thrd.datasender_cxt.DataSndCtl->SyncRepQueue), &(t_thrd.proc->dataSyncRepLinks));\r\n}",
		"comment":"/*\r\n * Insert t_thrd.proc into the specified SyncRepQueue\r\n */",
		"filename":"D:\\标注竞赛\\openGauss-server\\src\\gausskernel\\storage\\replication\\datasyncrep.cpp",
		"name":"DataSyncRepQueueInsert"
	},
	"DataSyncRepReleaseWaiters":{
		"body":"void DataSyncRepReleaseWaiters(void)\r\n{\r\n    volatile DataSndCtlData* datasndctl = t_thrd.datasender_cxt.DataSndCtl;\r\n    DataQueuePtr minOffSet;\r\n    int numreceive = 0;\r\n\r\n    (void)LWLockAcquire(DataSyncRepLock, LW_EXCLUSIVE);\r\n\r\n    minOffSet = GetMinReplyOffset();\r\n\r\n    SpinLockAcquire(&datasndctl->mutex);\r\n    if (DQByteLT(datasndctl->queue_offset, minOffSet)) {\r\n        datasndctl->queue_offset.queueid = minOffSet.queueid;\r\n        datasndctl->queue_offset.queueoff = minOffSet.queueoff;\r\n        SpinLockRelease(&datasndctl->mutex);\r\n\r\n        PopFromDataQueue(((DataSndCtlData*)datasndctl)->queue_offset, t_thrd.dataqueue_cxt.DataSenderQueue);\r\n\r\n        numreceive = DataSyncRepWakeQueue();\r\n    } else\r\n        SpinLockRelease(&datasndctl->mutex);\r\n\r\n    LWLockRelease(DataSyncRepLock);\r\n\r\n    ereport(DEBUG3,\r\n        (errmsg(\"released %d procs up to receive %u/%u\",\r\n            numreceive,\r\n            t_thrd.datasender_cxt.MyDataSnd->receivePosition.queueid,\r\n            t_thrd.datasender_cxt.MyDataSnd->receivePosition.queueoff)));\r\n}",
		"comment":"/*\r\n * Update the offset on each queue based upon our latest state. This\r\n * implements a simple policy of first-valid-standby-releases-waiter.\r\n *\r\n * Other policies are possible, which would change what we do here and what\r\n * perhaps also which information we store as well.\r\n */",
		"filename":"D:\\标注竞赛\\openGauss-server\\src\\gausskernel\\storage\\replication\\datasyncrep.cpp",
		"name":"DataSyncRepReleaseWaiters"
	},
	"DataSyncRepWakeQueue":{
		"body":"static int DataSyncRepWakeQueue(void)\r\n{\r\n    volatile DataSndCtlData* datasndctl = t_thrd.datasender_cxt.DataSndCtl;\r\n    PGPROC* proc = NULL;\r\n    PGPROC* thisproc = NULL;\r\n    int numprocs = 0;\r\n\r\n    Assert(SyncRepQueueIsOrderedByOffset());\r\n\r\n    proc = (PGPROC*)SHMQueueNext(&(t_thrd.datasender_cxt.DataSndCtl->SyncRepQueue),\r\n        &(t_thrd.datasender_cxt.DataSndCtl->SyncRepQueue),\r\n        offsetof(PGPROC, dataSyncRepLinks));\r\n\r\n    while (proc != NULL) {\r\n        /*\r\n         * Assume the queue is ordered by offset\r\n         */\r\n        if (DQByteLT(datasndctl->queue_offset, proc->waitDataSyncPoint))\r\n            return numprocs;\r\n\r\n        /*\r\n         * Move to next proc, so we can delete thisproc from the queue.\r\n         * thisproc is valid, proc may be NULL after this.\r\n         */\r\n        thisproc = proc;\r\n        proc = (PGPROC*)SHMQueueNext(&(t_thrd.datasender_cxt.DataSndCtl->SyncRepQueue),\r\n            &(proc->dataSyncRepLinks),\r\n            offsetof(PGPROC, dataSyncRepLinks));\r\n        /*\r\n         * Remove thisproc from queue.\r\n         */\r\n        SHMQueueDelete(&(thisproc->dataSyncRepLinks));\r\n\r\n        /*\r\n         * WaitForDataSync() reads dataSyncRepState without holding the lock, so\r\n         * make sure that it sees the queue link being removed before the\r\n         * dataSyncRepState change.\r\n         */\r\n        pg_write_barrier();\r\n\r\n        /*\r\n         * Set state to complete; see WaitForDataSync() for discussion of\r\n         * the various states.\r\n         */\r\n        thisproc->dataSyncRepState = SYNC_REP_WAIT_COMPLETE;\r\n\r\n        /*\r\n         * Wake only when we have set state and removed from queue.\r\n         */\r\n        SetLatch(&(thisproc->procLatch));\r\n\r\n        numprocs++;\r\n    }\r\n\r\n    return numprocs;\r\n}",
		"comment":"/*\r\n * Walk the specified queue from head. Set the state of any backends that\r\n * need to be woken, remove them from the queue, and then wake them.\r\n *\r\n * Must hold DataSyncRepLock.\r\n */",
		"filename":"D:\\标注竞赛\\openGauss-server\\src\\gausskernel\\storage\\replication\\datasyncrep.cpp",
		"name":"DataSyncRepWakeQueue"
	},
	"GetMinReplyOffset":{
		"body":"static DataQueuePtr GetMinReplyOffset(void)\r\n{\r\n    volatile DataSndCtlData* datasndctl = t_thrd.datasender_cxt.DataSndCtl;\r\n    DataQueuePtr slow = (DataQueuePtr){0, 0};\r\n    int i;\r\n    bool sender_has_invaild_position = false;\r\n\r\n    for (i = 0; i < g_instance.attr.attr_storage.max_wal_senders; i++) {\r\n        /* use volatile pointer to prevent code rearrangement */\r\n        volatile DataSnd* datasnd = &datasndctl->datasnds[i];\r\n\r\n        SpinLockAcquire(&datasnd->mutex);\r\n\r\n        if (datasnd->pid != 0 && datasnd->state > DATASNDSTATE_STARTUP && datasnd->sending) {\r\n            /*\r\n             * To find the minimum from the standby sender and the secondery\r\n             * sender. If datesnd1 receivePosition is 0, datesnd2 receivePosition\r\n             * is 500, the minimum should be 0;\r\n             */\r\n            if (DataQueuePtrIsInvalid(datasnd->receivePosition))\r\n                sender_has_invaild_position = true;\r\n\r\n            if (DataQueuePtrIsInvalid(slow) || DQByteLT(datasnd->receivePosition, slow)) {\r\n                slow.queueid = datasnd->receivePosition.queueid;\r\n                slow.queueoff = datasnd->receivePosition.queueoff;\r\n            }\r\n        }\r\n\r\n        SpinLockRelease(&datasnd->mutex);\r\n    }\r\n\r\n    if (sender_has_invaild_position)\r\n        slow = (DataQueuePtr){0, 0};\r\n\r\n    return slow;\r\n}",
		"comment":"/*\r\n * search the slow_offset last so that we can release up the queue;\r\n */",
		"filename":"D:\\标注竞赛\\openGauss-server\\src\\gausskernel\\storage\\replication\\datasyncrep.cpp",
		"name":"GetMinReplyOffset"
	},
	"WaitForDataSync":{
		"body":"void WaitForDataSync(void)\r\n{\r\n    int dataSyncRepState = SYNC_REP_WAITING;\r\n\r\n    /*\r\n     * Fast exit if user has not requested sync replication, or there are no\r\n     * sync replication standby names defined. Note that those standbys don't\r\n     * need to be connected.\r\n     */\r\n    if (!u_sess->attr.attr_storage.enable_stream_replication || !SyncRepRequested() || !SyncStandbysDefined() ||\r\n        (t_thrd.postmaster_cxt.HaShmData->current_mode == NORMAL_MODE)) {\r\n        ResetBCMArray();\r\n        return;\r\n    }\r\n\r\n    if (DataQueuePtrIsInvalid(t_thrd.proc->waitDataSyncPoint)) {\r\n        ResetBCMArray();\r\n        return;\r\n    }\r\n\r\n    AssertEreport(SHMQueueIsDetached(&(t_thrd.proc->dataSyncRepLinks)), MOD_FUNCTION, \"shm queue should be detached\");\r\n    AssertEreport(t_thrd.datasender_cxt.DataSndCtl != nullptr, MOD_FUNCTION, \"DataSndCtl should not be null\");\r\n\r\n    (void)LWLockAcquire(DataSyncRepLock, LW_EXCLUSIVE);\r\n    AssertEreport(t_thrd.proc->dataSyncRepState == SYNC_REP_NOT_WAITING, MOD_FUNCTION,\r\n        \"dataSyncRepState should be SYNC_REP_NOT_WAITING\");\r\n\r\n    if (DQByteLE(t_thrd.proc->waitDataSyncPoint, t_thrd.datasender_cxt.DataSndCtl->queue_offset)) {\r\n        LWLockRelease(DataSyncRepLock);\r\n        ClearBCMArray();\r\n        return;\r\n    }\r\n\r\n    /*\r\n     * Set our wait offset so DataSender will know when to wake us, and add ourselves to the queue.\r\n     */\r\n    t_thrd.proc->dataSyncRepState = SYNC_REP_WAITING;\r\n    DataSyncRepQueueInsert();\r\n    LWLockRelease(DataSyncRepLock);\r\n\r\n    ereport(DEBUG5,\r\n        (errmsg(\"WaitForDataSync:head2:%u/%u, tail1:%u/%u,tail2:%u/%u,wait:%u/%u\",\r\n            t_thrd.dataqueue_cxt.DataSenderQueue->use_head2.queueid,\r\n            t_thrd.dataqueue_cxt.DataSenderQueue->use_head2.queueoff,\r\n            t_thrd.dataqueue_cxt.DataSenderQueue->use_tail1.queueid,\r\n            t_thrd.dataqueue_cxt.DataSenderQueue->use_tail1.queueoff,\r\n            t_thrd.dataqueue_cxt.DataSenderQueue->use_tail2.queueid,\r\n            t_thrd.dataqueue_cxt.DataSenderQueue->use_tail2.queueoff,\r\n            t_thrd.proc->waitDataSyncPoint.queueid,\r\n            t_thrd.proc->waitDataSyncPoint.queueoff)));\r\n    WaitState oldStatus = pgstat_report_waitstatus(STATE_WAIT_DATASYNC);\r\n\r\n    /*\r\n     * Wait for reply_offset to be confirmed.\r\n     *\r\n     * Each proc has its own wait latch, so we perform a normal latch\r\n     * check/wait loop here.\r\n     */\r\n    for (;;) {\r\n        /* Must reset the latch before testing state. */\r\n        ResetLatch(&t_thrd.proc->procLatch);\r\n\r\n        /*\r\n         * Acquiring the lock is not needed, the latch ensures proper barriers.\r\n         * If it looks like we're done, we must really be done, because once\r\n         * walsender changes the state to SYNC_REP_WAIT_COMPLETE, it will never\r\n         * update it again, so we can't be seeing a stale value in that case.\r\n         */\r\n        dataSyncRepState = t_thrd.proc->dataSyncRepState;\r\n\r\n        if (dataSyncRepState == SYNC_REP_WAIT_COMPLETE) {\r\n            if (u_sess->attr.attr_storage.HaModuleDebug) {\r\n                ereport(LOG,\r\n                    (errmsg(\"HA-WaitForDataSync: waitpoint %u/%u done\",\r\n                        t_thrd.proc->waitDataSyncPoint.queueid,\r\n                        t_thrd.proc->waitDataSyncPoint.queueoff)));\r\n            }\r\n            break;\r\n        }\r\n\r\n        /*\r\n         * If a wait for synchronous replication is pending, we can neither\r\n         * acknowledge the commit nor raise ERROR or FATAL.  The latter would\r\n         * lead the client to believe that the transaction aborted, which\r\n         * is not true: it's already committed locally. The former is no good\r\n         * either: the client has requested synchronous replication, and is\r\n         * entitled to assume that an acknowledged commit is also replicated,\r\n         * which might not be true. So in this case we issue a WARNING (which\r\n         * some clients may be able to interpret) and shut off further output.\r\n         * We do NOT reset t_thrd.int_cxt.ProcDiePending, so that the process will die after\r\n         * the commit is cleaned up.\r\n         */\r\n        if (t_thrd.int_cxt.ProcDiePending || t_thrd.proc_cxt.proc_exit_inprogress) {\r\n            ereport(WARNING,\r\n                (errcode(ERRCODE_ADMIN_SHUTDOWN),\r\n                    errmsg(\"canceling the wait for synchronous replication and terminating connection due to \"\r\n                           \"administrator command\"),\r\n                    errdetail(\"The transaction has already committed locally, but might not have been replicated to \"\r\n                              \"the standby.\")));\r\n            t_thrd.postgres_cxt.whereToSendOutput = DestNone;\r\n            DataSyncRepCancelWait();\r\n            break;\r\n        }\r\n\r\n        /*\r\n         * It's unclear what to do if a query cancel interrupt arrives.  We\r\n         * can't actually abort at this point, but ignoring the interrupt\r\n         * altogether is not helpful, so we just terminate the wait with a\r\n         * suitable warning.\r\n         */\r\n        if (t_thrd.int_cxt.QueryCancelPending) {\r\n            t_thrd.int_cxt.QueryCancelPending = false;\r\n            ereport(WARNING,\r\n                (errmsg(\"canceling wait for synchronous replication due to user request\"),\r\n                    errdetail(\"The transaction has already committed locally, but might not have been replicated to \"\r\n                              \"the standby.\")));\r\n            DataSyncRepCancelWait();\r\n            break;\r\n        }\r\n\r\n        /*\r\n         * If the postmaster dies, we'll probably never get an\r\n         * acknowledgement, because all the wal sender processes will exit. So\r\n         * just bail out.\r\n         */\r\n        if (!PostmasterIsAlive()) {\r\n            t_thrd.int_cxt.ProcDiePending = true;\r\n            t_thrd.postgres_cxt.whereToSendOutput = DestNone;\r\n            DataSyncRepCancelWait();\r\n            break;\r\n        }\r\n\r\n        /*\r\n         * if we  modify the syncmode dynamically, we'll stop wait\r\n         */\r\n        if (t_thrd.walsender_cxt.WalSndCtl->sync_master_standalone ||\r\n            synchronous_commit <= SYNCHRONOUS_COMMIT_LOCAL_FLUSH) {\r\n            ereport(WARNING,\r\n                (errmsg(\"canceling wait for synchronous replication due to syncmaster standalone.\"),\r\n                    errdetail(\"The transaction has already committed locally, but might not have been replicated to \"\r\n                              \"the standby.\")));\r\n            DataSyncRepCancelWait();\r\n            break;\r\n        }\r\n\r\n        /*\r\n         * If the datasender to standby is offline, we'll stop wait.\r\n         */\r\n        if (IsCatchupProcess() && !DataSndInProgress(SNDROLE_PRIMARY_STANDBY)) {\r\n            ereport(WARNING, (errmsg(\"catchup canceling wait for synchronous replication due to datasender offline\")));\r\n            DataSyncRepCancelWait();\r\n            break;\r\n        }\r\n\r\n        /*\r\n         * Wait on latch.  Any condition that should wake us up will set the\r\n         * latch, so no need for timeout.\r\n         */\r\n        (void)WaitLatch(&t_thrd.proc->procLatch, WL_LATCH_SET | WL_POSTMASTER_DEATH | WL_TIMEOUT, 1000L);\r\n    }\r\n\r\n    (void)pgstat_report_waitstatus(oldStatus);\r\n\r\n    /*\r\n     * DataSender has checked our offset and has removed us from queue. Clean up\r\n     * state and leave.  It's OK to reset these shared memory fields without\r\n     * holding DataSyncRepLock, because any datasenders will ignore us anyway when\r\n     * we're not on the queue. We need a read barrier to make sure we see\r\n     * the changes to the queue link (this might be unnecessary without\r\n     * assertions, but better safe than sorry).\r\n     */\r\n    pg_read_barrier();\r\n    Assert(SHMQueueIsDetached(&(t_thrd.proc->dataSyncRepLinks)));\r\n    t_thrd.proc->dataSyncRepState = SYNC_REP_NOT_WAITING;\r\n    t_thrd.proc->waitDataSyncPoint.queueid = 0;\r\n    t_thrd.proc->waitDataSyncPoint.queueoff = 0;\r\n\r\n    /*\r\n     * when the data has been send to standby, we should clear the excess bcm blocks.\r\n     */\r\n    if (dataSyncRepState == SYNC_REP_WAIT_COMPLETE)\r\n        ClearBCMArray();\r\n\r\n    /*\r\n     * After the transaction is finished or cancelled, we should reset the BCMArray.\r\n     * In case of the transaction is cancelled, and the table is dropped, then to clear\r\n     * the BCM maybe encounter ERROR.\r\n     */\r\n    ResetBCMArray();\r\n}",
		"comment":"",
		"filename":"D:\\标注竞赛\\openGauss-server\\src\\gausskernel\\storage\\replication\\datasyncrep.cpp",
		"name":"WaitForDataSync"
	}
}